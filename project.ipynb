{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-forth",
   "metadata": {},
   "source": [
    "# TAL - Classification de dépêches d’agence avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signed-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics.scores import (precision, recall, f_measure)\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "decent-dressing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['JAPAN', 'BUYS', 'CANADIAN', 'RAPESEED', 'Japanese', ...],\n",
       " ['oilseed', 'rapeseed'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "documents = []\n",
    "# Loop through each file id and collect each files categories and tokenized words\n",
    "for file in fileids:\n",
    "    words = reuters.words(file)\n",
    "    documents.append((words, reuters.categories(file)))\n",
    "\n",
    "shuffle(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-qualification",
   "metadata": {},
   "source": [
    "> Note: We've noticed that in the tokenized words of the corpus, the word `U.S` is split into three tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-tuition",
   "metadata": {},
   "source": [
    "## Classifieurs binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "complete-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_frequence):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_frequence:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def most_freq_words(documents, limit=2000):\n",
    "    all_words = nltk.FreqDist(w\n",
    "        for document in documents\n",
    "        for w in document[0]\n",
    "    )\n",
    "    return list(all_words)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "sufficient-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(documents, tag, feature_extractor, **kwargs):\n",
    "    if 'to_lower' in kwargs and kwargs['to_lower']:\n",
    "        documents = list(map(lambda d: (list(map(str.lower, d[0])), d[1]), documents))\n",
    "\n",
    "    if 'lemmatizer' in kwargs:\n",
    "        lemmatizer = kwargs['lemmatizer']\n",
    "        documents = list(map(lambda d: (list(map(lemmatizer.lemmatize, d[0])), d[1]), documents))\n",
    "    \n",
    "    if 'stopwords' in kwargs:\n",
    "        stopwords = set(kwargs['stopwords'])\n",
    "        documents = list(map(\n",
    "            lambda d: (\n",
    "                list(filter(lambda w: not w.lower() in stopwords and w[0].isalnum(), d[0])), \n",
    "                d[1]\n",
    "            ), documents))\n",
    "        \n",
    "    analyzer_res = []\n",
    "    if 'analyzer' in kwargs:\n",
    "        analyzer_res = kwargs['analyzer'](documents)\n",
    "\n",
    "    dataset = []\n",
    "    for document in documents:\n",
    "        dataset.append((feature_extractor(document[0], analyzer_res), tag in document[1]))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    split_ratio = 0.6\n",
    "    split_ratio2 = 0.8\n",
    "    \n",
    "    split = int(len(dataset) * split_ratio)\n",
    "    split2 = int(len(dataset) * split_ratio2)\n",
    "\n",
    "    return (dataset[:split], dataset[split:split2], dataset[split2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "isolated-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_classifier(documents, tag, hyperparams):\n",
    "    print('Finding best classifier for {}'.format(tag))\n",
    "    print('----------')\n",
    "\n",
    "    best = (None, 0.0)\n",
    "    for hyperparam in hyperparams:\n",
    "        dataset = create_dataset(documents, 'money-fx', **hyperparam)\n",
    "        train_set, test_set, dev_set = split_dataset(dataset)\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        acc = nltk.classify.accuracy(classifier, dev_set)\n",
    "        \n",
    "        if acc > best[1]:\n",
    "            best = (classifier, acc)\n",
    "        \n",
    "        print('Accuracy using \"{}\": {:.2f}%'.format(hyperparam['title'], acc*100))\n",
    "    return (best[0], test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-darwin",
   "metadata": {},
   "source": [
    "### Classification des documents `money-fx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "unsigned-shooting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for money-supply\n",
      "----------\n",
      "Accuracy using \"Most frequent words\": 89.57%\n",
      "Accuracy using \"Lowered most frequent words\": 88.55%\n",
      "Accuracy using \"Lemmatized most frequent words\": 88.00%\n",
      "Accuracy using \"Most frequent words without stopwords\": 90.64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparams = [\n",
    "    {\n",
    "        'title': 'Most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lowered most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lemmatized most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "]\n",
    "\n",
    "best_classifier(documents, 'money-supply', hyperparams)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "public-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for money-supply\n",
      "----------\n",
      "Accuracy using \"Lowered most frequent words without stopwords\": 90.59%\n",
      "Accuracy using \"Lemmatized most frequent words without stopwords\": 92.96%\n",
      "Accuracy using \"Lowered lemmatized most frequent words without stopwords\": 91.57%\n"
     ]
    }
   ],
   "source": [
    "hyperparams_wo_sw = [\n",
    "    {\n",
    "        'title': 'Lowered most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lemmatized most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lowered lemmatized most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "]\n",
    "\n",
    "classifier_moneyfx, moneyfx_testset = best_classifier(documents, 'money-supply', hyperparams_wo_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-convenience",
   "metadata": {},
   "source": [
    "### Classification des documents `wheat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fundamental-latino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for wheat\n",
      "----------\n",
      "Accuracy using \"Most frequent words\": 89.39%\n",
      "Accuracy using \"Lowered most frequent words\": 89.06%\n",
      "Accuracy using \"Lemmatized most frequent words\": 89.25%\n",
      "Accuracy using \"Most frequent words without stopwords\": 91.43%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparams = [\n",
    "    {\n",
    "        'title': 'Most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lowered most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lemmatized most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "]\n",
    "\n",
    "best_classifier(documents, 'wheat', hyperparams)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "regular-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for wheat\n",
      "----------\n",
      "Accuracy using \"Lowered most frequent words without stopwords\": 90.50%\n",
      "Accuracy using \"Lemmatized most frequent words without stopwords\": 91.33%\n",
      "Accuracy using \"Lowered lemmatized most frequent words without stopwords\": 91.15%\n"
     ]
    }
   ],
   "source": [
    "hyperparams_wo_sw = [\n",
    "    {\n",
    "        'title': 'Lowered most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lemmatized most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lowered lemmatized most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "]\n",
    "\n",
    "classifier_wheat, wheat_testset = best_classifier(documents, 'wheat', hyperparams_wo_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-costa",
   "metadata": {},
   "source": [
    "### Classification des documents `gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "duplicate-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for gold\n",
      "----------\n",
      "Accuracy using \"Most frequent words\": 88.97%\n",
      "Accuracy using \"Lowered most frequent words\": 88.37%\n",
      "Accuracy using \"Lemmatized most frequent words\": 87.49%\n",
      "Accuracy using \"Most frequent words without stopwords\": 90.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparams = [\n",
    "    {\n",
    "        'title': 'Most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lowered most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lemmatized most frequent words',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "]\n",
    "\n",
    "best_classifier(documents, 'gold', hyperparams)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "proper-balance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for gold\n",
      "----------\n",
      "Accuracy using \"Lowered most frequent words without stopwords\": 91.66%\n",
      "Accuracy using \"Lemmatized most frequent words without stopwords\": 91.47%\n",
      "Accuracy using \"Lowered lemmatized most frequent words without stopwords\": 92.08%\n"
     ]
    }
   ],
   "source": [
    "hyperparams_wo_sw = [\n",
    "    {\n",
    "        'title': 'Lowered most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lemmatized most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lowered lemmatized most frequent words without stopwords',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "]\n",
    "\n",
    "classifier_gold, gold_testset = best_classifier(documents, 'gold', hyperparams_wo_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "unlimited-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_test_sets(testset, classifier):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (feats, label) in enumerate(testset):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    return refsets, testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "smooth-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "moneyfx_refsets, moneyfx_testsets = ref_test_sets(moneyfx_testset, classifier_moneyfx)\n",
    "wheat_refsets, wheat_testsets = ref_test_sets(wheat_testset, classifier_wheat)\n",
    "gold_refsets, gold_testsets = ref_test_sets(gold_testset, classifier_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "silent-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Money-fx:\n",
      "---------\n",
      "Precision: 0.34563758389261745\n",
      "Recall: 0.7744360902255639\n",
      "F-mesure: 0.47795823665893267\n",
      "\n",
      "Wheat:\n",
      "---------\n",
      "Precision: 0.38924050632911394\n",
      "Recall: 0.7884615384615384\n",
      "F-mesure: 0.5211864406779662\n",
      "\n",
      "Gold:\n",
      "---------\n",
      "Precision: 0.36824324324324326\n",
      "Recall: 0.8257575757575758\n",
      "F-mesure: 0.5093457943925235\n"
     ]
    }
   ],
   "source": [
    "print('Money-fx:')\n",
    "print('---------')\n",
    "print('Precision:', precision(moneyfx_refsets[True], moneyfx_testsets[True]))\n",
    "print('Recall:'   , recall(moneyfx_refsets[True], moneyfx_testsets[True]))\n",
    "print('F-mesure:' , f_measure(moneyfx_refsets[True], moneyfx_testsets[True]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Wheat:')\n",
    "print('---------')\n",
    "print('Precision:', precision(wheat_refsets[True], wheat_testsets[True]))\n",
    "print('Recall:'   , recall(wheat_refsets[True], wheat_testsets[True]))\n",
    "print('F-mesure:' , f_measure(wheat_refsets[True], wheat_testsets[True]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Gold:')\n",
    "print('---------')\n",
    "print('Precision:', precision(gold_refsets[True], gold_testsets[True]))\n",
    "print('Recall:'   , recall(gold_refsets[True], gold_testsets[True]))\n",
    "print('F-mesure:' , f_measure(gold_refsets[True], gold_testsets[True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-michigan",
   "metadata": {},
   "source": [
    "## Classifieur multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-justice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-berlin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
