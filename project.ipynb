{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-forth",
   "metadata": {},
   "source": [
    "# TAL - Classification de dépêches d’agence avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signed-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decent-dressing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['FLEET', '&', 'lt', ';', 'FLT', '>', 'COULD', 'FACE', ...], ['acq'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "documents = []\n",
    "# Loop through each file id and collect each files categories and tokenized words\n",
    "for file in fileids:\n",
    "    words = reuters.words(file)\n",
    "    documents.append((words, reuters.categories(file)))\n",
    "\n",
    "shuffle(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-qualification",
   "metadata": {},
   "source": [
    "> Note: We've noticed that in the tokenized words of the corpus, the word `U.S` is split into three tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-tuition",
   "metadata": {},
   "source": [
    "## Classifieurs binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-darwin",
   "metadata": {},
   "source": [
    "### Classification des documents `money-fx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "controversial-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_frequence):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_frequence:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def most_freq_words(documents, limit=2000):\n",
    "    all_words = nltk.FreqDist(w\n",
    "        for document in documents\n",
    "        for w in document[0]\n",
    "    )\n",
    "    return list(all_words)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cultural-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(documents, tag, feature_extractor, **kwargs):\n",
    "    if 'to_lower' in kwargs and kwargs['to_lower']:\n",
    "        documents = list(map(lambda d: (list(map(str.lower, d[0])), d[1]), documents))\n",
    "\n",
    "    if 'lemmatizer' in kwargs:\n",
    "        lemmatizer = kwargs['lemmatizer']\n",
    "        documents = list(map(lambda d: (list(map(lemmatizer.lemmatize, d[0])), d[1]), documents))\n",
    "    \n",
    "    if 'stopwords' in kwargs:\n",
    "        stopwords = set(kwargs['stopwords'])\n",
    "        documents = list(map(\n",
    "            lambda d: (\n",
    "                list(filter(lambda w: not w.lower() in stopwords and w[0].isalnum(), d[0])), \n",
    "                d[1]\n",
    "            ), documents))\n",
    "        \n",
    "    analyzer_res = []\n",
    "    if 'analyzer' in kwargs:\n",
    "        analyzer_res = kwargs['analyzer'](documents)\n",
    "\n",
    "    dataset = []\n",
    "    for document in documents:\n",
    "        dataset.append((feature_extractor(document[0], analyzer_res), tag in document[1]))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    split_ratio = 0.6\n",
    "    split_ratio2 = 0.8\n",
    "    \n",
    "    split = int(len(dataset) * split_ratio)\n",
    "    split2 = int(len(dataset) * split_ratio2)\n",
    "\n",
    "    return (dataset[:split], dataset[split:split2], dataset[split2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unsigned-shooting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8730305838739574\n",
      "0.8748841519925857\n"
     ]
    }
   ],
   "source": [
    "hyperparams = [\n",
    "    {\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "    },\n",
    "    {\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "    },\n",
    "    {\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer()\n",
    "    },\n",
    "    {\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'stopwords': stopwords.words('english')\n",
    "    },\n",
    "]\n",
    "\n",
    "for hyperparam in hyperparams:\n",
    "    dataset = create_dataset(documents, 'money-fx', **hyperparam)\n",
    "    train_set, test_set, dev_set = split_dataset(dataset)\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    print(nltk.classify.accuracy(classifier, dev_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-convenience",
   "metadata": {},
   "source": [
    "### Deuxième classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-costa",
   "metadata": {},
   "source": [
    "### Troisième classifieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "looking-michigan",
   "metadata": {},
   "source": [
    "## Classifieur multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-justice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-berlin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
