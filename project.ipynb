{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-forth",
   "metadata": {},
   "source": [
    "# TAL - Classification de dépêches d’agence avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signed-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics.scores import (precision, recall, f_measure)\n",
    "\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decent-dressing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['NORTHERN', 'TELECOM', 'PROPOSES', 'TWO', '-', 'FOR', ...], ['earn'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "documents = []\n",
    "# Loop through each file id and collect each files categories and tokenized words\n",
    "for file in fileids:\n",
    "    words = reuters.words(file)\n",
    "    documents.append((words, reuters.categories(file)))\n",
    "\n",
    "shuffle(documents)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-qualification",
   "metadata": {},
   "source": [
    "Nous avons remarqué que dans la tokenisation des mots du corpus, le mots `U.S` est séparé en trois tokens distinct, `U`, `.` et `S`. Nous avon estimé que dans le cadre de ce labo, cela ne devrait pas causer trop de problèmes et nous avons donc laissé cette séparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-tuition",
   "metadata": {},
   "source": [
    "## Classifieur binaire\n",
    "\n",
    "Pour la classification des documents, nous avons décidé d'utiliser la fréquence des mots. Nous avons donc commencé par déterminer la fréquence de **TOUT** les mots du dataset (i.e. tout les documents), puis les `2000` mots les plus fréquents sont retourné.\n",
    "\n",
    "> Note: La limite de la fréquence des mots que la fonction retourne est paramètrable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complete-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_frequence):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_frequence:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def most_freq_words(documents, limit=2000):\n",
    "    all_words = nltk.FreqDist(w\n",
    "        for document in documents\n",
    "        for w in document[0]\n",
    "    )\n",
    "    return list(all_words)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sufficient-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(documents, tag, feature_extractor, **kwargs):\n",
    "    if 'to_lower' in kwargs and kwargs['to_lower']:\n",
    "        documents = list(map(lambda d: (list(map(str.lower, d[0])), d[1]), documents))\n",
    "\n",
    "    if 'lemmatizer' in kwargs:\n",
    "        lemmatizer = kwargs['lemmatizer']\n",
    "        documents = list(map(lambda d: (list(map(lemmatizer.lemmatize, d[0])), d[1]), documents))\n",
    "    \n",
    "    if 'stopwords' in kwargs:\n",
    "        stopwords = set(kwargs['stopwords'])\n",
    "        documents = list(map(\n",
    "            lambda d: (\n",
    "                list(filter(lambda w: not w.lower() in stopwords and w[0].isalnum(), d[0])), \n",
    "                d[1]\n",
    "            ), documents))\n",
    "        \n",
    "    analyzer_res = []\n",
    "    if 'analyzer' in kwargs:\n",
    "        analyzer_res = kwargs['analyzer'](documents)\n",
    "\n",
    "    dataset = []\n",
    "    for document in documents:\n",
    "        dataset.append((feature_extractor(document[0], analyzer_res), tag in document[1]))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    split_ratio = 0.6\n",
    "    split_ratio2 = 0.8\n",
    "    \n",
    "    split = int(len(dataset) * split_ratio)\n",
    "    split2 = int(len(dataset) * split_ratio2)\n",
    "\n",
    "    return (dataset[:split], dataset[split:split2], dataset[split2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "isolated-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_classifier(documents, tag, dataset_creator, hyperparams):\n",
    "    print('Finding best classifier for {}'.format(tag))\n",
    "    print('----------')\n",
    "\n",
    "    best = (None, 0.0)\n",
    "    for hyperparam in hyperparams:\n",
    "        dataset = dataset_creator(documents, tag, **hyperparam)\n",
    "        train_set, test_set, dev_set = split_dataset(dataset)\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        acc = nltk.classify.accuracy(classifier, dev_set)\n",
    "        \n",
    "        if acc > best[1]:\n",
    "            best = (classifier, acc)\n",
    "        \n",
    "        print('Accuracy using \"{}\": {:.2f}%'.format(hyperparam['title'], acc*100))\n",
    "    return (best[0], test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-morning",
   "metadata": {},
   "source": [
    "### Combinaison des différents hyperparamètres\n",
    "\n",
    "TODO explain the hyperparam we're going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "correct-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = [\n",
    "    {\n",
    "        'title': 'To lower: no, Lemmatize: no, No stopwords: no',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: yes, Lemmatize: no, No stopwords: no',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: no, Lemmatize: yes, No stopwords: no',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: no, Lemmatize: no, No stopwords: yes',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: yes, Lemmatize: no, No stopwords: yes',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: no, Lemmatize: yes, No stopwords: yes',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "        'stopwords': stopwords.words('english'),\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: yes, Lemmatize: yes, No stopwords: no',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "    {\n",
    "        'title': 'To lower: yes, Lemmatize: yes, No stopwords: yes',\n",
    "        'feature_extractor': document_features,\n",
    "        'analyzer': most_freq_words,\n",
    "        'to_lower': True,\n",
    "        'stopwords': stopwords.words('english'),\n",
    "        'lemmatizer': WordNetLemmatizer(),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-darwin",
   "metadata": {},
   "source": [
    "### Classification des documents `money-fx`\n",
    "TODO Comment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "public-islam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for money-fx\n",
      "----------\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: no\": 87.72%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: no\": 88.74%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: no\": 87.86%\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: yes\": 90.73%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: yes\": 90.55%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: yes\": 90.64%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: no\": 87.53%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: yes\": 91.71%\n"
     ]
    }
   ],
   "source": [
    "classifier_moneyfx, moneyfx_testset = best_classifier(documents, 'money-fx', create_dataset, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-convenience",
   "metadata": {},
   "source": [
    "### Classification des documents `wheat`\n",
    "TODO Comment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regular-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for wheat\n",
      "----------\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: no\": 89.99%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: no\": 89.76%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: no\": 89.99%\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: yes\": 93.51%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: yes\": 94.86%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: yes\": 93.23%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: no\": 90.59%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: yes\": 94.35%\n"
     ]
    }
   ],
   "source": [
    "classifier_wheat, wheat_testset = best_classifier(documents, 'wheat', create_dataset, hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-costa",
   "metadata": {},
   "source": [
    "### Classification des documents `gold`\n",
    "TODO Comment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "proper-balance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for gold\n",
      "----------\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: no\": 93.79%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: no\": 95.09%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: no\": 96.71%\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: yes\": 98.75%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: yes\": 98.42%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: yes\": 98.05%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: no\": 98.19%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: yes\": 98.56%\n"
     ]
    }
   ],
   "source": [
    "classifier_gold, gold_testset = best_classifier(documents, 'gold', create_dataset, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unlimited-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_test_sets(testset, classifier):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "\n",
    "    for i, (feats, label) in enumerate(testset):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "\n",
    "    return refsets, testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "smooth-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "moneyfx_refsets, moneyfx_testsets = ref_test_sets(moneyfx_testset, classifier_moneyfx)\n",
    "wheat_refsets, wheat_testsets = ref_test_sets(wheat_testset, classifier_wheat)\n",
    "gold_refsets, gold_testsets = ref_test_sets(gold_testset, classifier_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "silent-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Money-fx:\n",
      "---------\n",
      "Precision: 0.3639344262295082\n",
      "Recall: 0.8283582089552238\n",
      "F-mesure: 0.5056947608200456\n",
      "\n",
      "Wheat:\n",
      "---------\n",
      "Precision: 0.32098765432098764\n",
      "Recall: 0.9285714285714286\n",
      "F-mesure: 0.4770642201834862\n",
      "\n",
      "Gold:\n",
      "---------\n",
      "Precision: 0.5306122448979592\n",
      "Recall: 0.896551724137931\n",
      "F-mesure: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Money-fx:')\n",
    "print('---------')\n",
    "print('Precision:', precision(moneyfx_refsets[True], moneyfx_testsets[True]))\n",
    "print('Recall:'   , recall(moneyfx_refsets[True], moneyfx_testsets[True]))\n",
    "print('F-mesure:' , f_measure(moneyfx_refsets[True], moneyfx_testsets[True]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Wheat:')\n",
    "print('---------')\n",
    "print('Precision:', precision(wheat_refsets[True], wheat_testsets[True]))\n",
    "print('Recall:'   , recall(wheat_refsets[True], wheat_testsets[True]))\n",
    "print('F-mesure:' , f_measure(wheat_refsets[True], wheat_testsets[True]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Gold:')\n",
    "print('---------')\n",
    "print('Precision:', precision(gold_refsets[True], gold_testsets[True]))\n",
    "print('Recall:'   , recall(gold_refsets[True], gold_testsets[True]))\n",
    "print('F-mesure:' , f_measure(gold_refsets[True], gold_testsets[True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-michigan",
   "metadata": {},
   "source": [
    "## Classifieur multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sharp-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_dataset(documents, tags, feature_extractor, **kwargs):\n",
    "    if 'to_lower' in kwargs and kwargs['to_lower']:\n",
    "        documents = list(map(lambda d: (list(map(str.lower, d[0])), d[1]), documents))\n",
    "\n",
    "    if 'lemmatizer' in kwargs:\n",
    "        lemmatizer = kwargs['lemmatizer']\n",
    "        documents = list(map(lambda d: (list(map(lemmatizer.lemmatize, d[0])), d[1]), documents))\n",
    "    \n",
    "    if 'stopwords' in kwargs:\n",
    "        stopwords = set(kwargs['stopwords'])\n",
    "        documents = list(map(\n",
    "            lambda d: (\n",
    "                list(filter(lambda w: not w.lower() in stopwords and w[0].isalnum(), d[0])), \n",
    "                d[1]\n",
    "            ), documents))\n",
    "        \n",
    "    analyzer_res = []\n",
    "    if 'analyzer' in kwargs:\n",
    "        analyzer_res = kwargs['analyzer'](documents)\n",
    "\n",
    "    dataset = []\n",
    "    for document in documents:\n",
    "        document_tags = list(set(tags).intersection(document[1]))\n",
    "        tag = 'other' if document_tags == [] else document_tags[0]\n",
    "\n",
    "        dataset.append((feature_extractor(document[0], analyzer_res), tag))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "connected-berlin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = [{\n",
    "    'title': 'mdr',\n",
    "    'feature_extractor': document_features,\n",
    "    'analyzer': most_freq_words,\n",
    "}]\n",
    "\n",
    "f = ['money-fx', 'wheat', 'gold']\n",
    "#create_multi_dataset(documents, f, **params)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "homeless-invalid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best classifier for ['money-fx', 'wheat', 'gold']\n",
      "----------\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: no\": 80.77%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: no\": 82.90%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: no\": 81.84%\n",
      "Accuracy using \"To lower: no, Lemmatize: no, No stopwords: yes\": 84.62%\n",
      "Accuracy using \"To lower: yes, Lemmatize: no, No stopwords: yes\": 86.42%\n",
      "Accuracy using \"To lower: no, Lemmatize: yes, No stopwords: yes\": 85.31%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: no\": 80.63%\n",
      "Accuracy using \"To lower: yes, Lemmatize: yes, No stopwords: yes\": 86.93%\n"
     ]
    }
   ],
   "source": [
    "classifier_multiclass, multiclass_testset = best_classifier(documents, ['money-fx', 'wheat', 'gold'], create_multi_dataset, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "promising-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro / macro average?\n",
    "# calculate precision, recall & f-score for each tag in multi class\n",
    "# compare result with corresponding binary classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
